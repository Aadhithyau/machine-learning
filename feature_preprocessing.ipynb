{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qBBCz5TD3B7H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "SCALERS = {\n",
        "    \"StandardScaler\": StandardScaler(),\n",
        "    \"MinMaxScaler\": MinMaxScaler(),\n",
        "    \"RobustScaler\": RobustScaler(),\n",
        "    \"MaxAbsScaler\": MaxAbsScaler()\n",
        "}\n",
        "\n",
        "def run_scaler_experiment_classification(X, y, title):\n",
        "    \"\"\"\n",
        "    Applies 4 scalers to X, trains 3 models (NB, DT, ANN) and prints accuracy for each.\n",
        "    \"\"\"\n",
        "    print(f\"\\n\\n==============================\")\n",
        "    print(f\"DATASET: {title}\")\n",
        "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    # Split once (fair comparison)\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        X, y, test_size=0.25, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    # Models used in this chat\n",
        "    models = {\n",
        "        \"NaiveBayes(GaussianNB)\": GaussianNB(),\n",
        "        \"DecisionTree(entropy)\": DecisionTreeClassifier(criterion=\"entropy\", random_state=42),\n",
        "        \"ANN(MLP)\": MLPClassifier(hidden_layer_sizes=(64, 32), activation=\"relu\",\n",
        "                                  max_iter=500, random_state=42)\n",
        "    }\n",
        "\n",
        "    for scaler_name, scaler in SCALERS.items():\n",
        "        print(f\"\\n--- Scaler: {scaler_name} ---\")\n",
        "\n",
        "        # Scale X (fit only on train, transform test)\n",
        "        X_tr_s = scaler.fit_transform(X_tr)\n",
        "        X_te_s = scaler.transform(X_te)\n",
        "\n",
        "        for model_name, model in models.items():\n",
        "            model.fit(X_tr_s, y_tr)\n",
        "            pred = model.predict(X_te_s)\n",
        "            acc = accuracy_score(y_te, pred)\n",
        "            print(f\"{model_name:22s} Accuracy = {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------- Load Diabetes dataset (Pima Indians) --------\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "cols = [\"Pregnancies\",\"Glucose\",\"BloodPressure\",\"SkinThickness\",\n",
        "        \"Insulin\",\"BMI\",\"DiabetesPedigreeFunction\",\"Age\",\"Outcome\"]\n",
        "df = pd.read_csv(url, names=cols)\n",
        "\n",
        "X = df.drop(columns=[\"Outcome\"])\n",
        "y = df[\"Outcome\"].astype(int)\n",
        "\n",
        "# Medical rule: 0 values in these columns are not realistic -> treat as missing\n",
        "zero_cols = [\"Glucose\",\"BloodPressure\",\"SkinThickness\",\"Insulin\",\"BMI\"]\n",
        "X[zero_cols] = X[zero_cols].replace(0, np.nan)\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "run_scaler_experiment_classification(X, y, \"Diabetes (Pima Indians)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llkt8vOA3H2F",
        "outputId": "3d5779d9-0732-4744-e94a-afe12ef9ae5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================\n",
            "DATASET: Diabetes (Pima Indians)\n",
            "X shape: (768, 8), y shape: (768,)\n",
            "==============================\n",
            "\n",
            "--- Scaler: StandardScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7240\n",
            "DecisionTree(entropy)  Accuracy = 0.7031\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN(MLP)               Accuracy = 0.7552\n",
            "\n",
            "--- Scaler: MinMaxScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7240\n",
            "DecisionTree(entropy)  Accuracy = 0.7135\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN(MLP)               Accuracy = 0.7083\n",
            "\n",
            "--- Scaler: RobustScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7240\n",
            "DecisionTree(entropy)  Accuracy = 0.7188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN(MLP)               Accuracy = 0.7396\n",
            "\n",
            "--- Scaler: MaxAbsScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7240\n",
            "DecisionTree(entropy)  Accuracy = 0.7083\n",
            "ANN(MLP)               Accuracy = 0.7240\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0=malignant, 1=benign\n",
        "\n",
        "# (Usually no missing values, but safe to keep imputer)\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "run_scaler_experiment_classification(X, y, \"Breast Cancer (Wisconsin)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em1AP3n_3Uuj",
        "outputId": "20e75b46-065d-49b4-faa4-8657f6c06618"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================\n",
            "DATASET: Breast Cancer (Wisconsin)\n",
            "X shape: (569, 30), y shape: (569,)\n",
            "==============================\n",
            "\n",
            "--- Scaler: StandardScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.9371\n",
            "DecisionTree(entropy)  Accuracy = 0.9301\n",
            "ANN(MLP)               Accuracy = 0.9720\n",
            "\n",
            "--- Scaler: MinMaxScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.9371\n",
            "DecisionTree(entropy)  Accuracy = 0.9301\n",
            "ANN(MLP)               Accuracy = 0.9790\n",
            "\n",
            "--- Scaler: RobustScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.9371\n",
            "DecisionTree(entropy)  Accuracy = 0.9301\n",
            "ANN(MLP)               Accuracy = 0.9720\n",
            "\n",
            "--- Scaler: MaxAbsScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.9371\n",
            "DecisionTree(entropy)  Accuracy = 0.9301\n",
            "ANN(MLP)               Accuracy = 0.9860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "X_raw, y_raw = fetch_openml(name=\"car\", version=1, as_frame=True, return_X_y=True)\n",
        "\n",
        "df = X_raw.copy()\n",
        "df[\"class\"] = y_raw\n",
        "\n",
        "# Encode features\n",
        "df[\"buying\"]   = df[\"buying\"].map({\"low\":0, \"med\":1, \"high\":2, \"vhigh\":3})\n",
        "df[\"maint\"]    = df[\"maint\"].map({\"low\":0, \"med\":1, \"high\":2, \"vhigh\":3})\n",
        "df[\"doors\"]    = df[\"doors\"].map({\"2\":2, \"3\":3, \"4\":4, \"5more\":5})\n",
        "df[\"persons\"]  = df[\"persons\"].map({\"2\":2, \"4\":4, \"more\":5})\n",
        "df[\"lug_boot\"] = df[\"lug_boot\"].map({\"small\":0, \"med\":1, \"big\":2})\n",
        "df[\"safety\"]   = df[\"safety\"].map({\"low\":0, \"med\":1, \"high\":2})\n",
        "\n",
        "# Encode target\n",
        "df[\"target\"] = df[\"class\"].map({\"unacc\":0, \"acc\":1, \"good\":2, \"vgood\":3})\n",
        "\n",
        "X = df.drop(columns=[\"class\",\"target\"]).values\n",
        "y = df[\"target\"].astype(int).values\n",
        "\n",
        "# Safe impute (if any)\n",
        "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "run_scaler_experiment_classification(X, y, \"Car Evaluation\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ib0ceEz3alh",
        "outputId": "64d89cdd-9096-4f48-c96c-bf48c3055f5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/datasets/_openml.py:1030: UserWarning: Version 1 of dataset car is inactive, meaning that issues have been found in the dataset. Try using a newer version from this URL: https://openml.org/data/v1/download/21/car.arff\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "==============================\n",
            "DATASET: Car Evaluation\n",
            "X shape: (1728, 6), y shape: (1728,)\n",
            "==============================\n",
            "\n",
            "--- Scaler: StandardScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7523\n",
            "DecisionTree(entropy)  Accuracy = 0.9676\n",
            "ANN(MLP)               Accuracy = 0.9931\n",
            "\n",
            "--- Scaler: MinMaxScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7523\n",
            "DecisionTree(entropy)  Accuracy = 0.9676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANN(MLP)               Accuracy = 0.9815\n",
            "\n",
            "--- Scaler: RobustScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7546\n",
            "DecisionTree(entropy)  Accuracy = 0.9653\n",
            "ANN(MLP)               Accuracy = 0.9931\n",
            "\n",
            "--- Scaler: MaxAbsScaler ---\n",
            "NaiveBayes(GaussianNB) Accuracy = 0.7523\n",
            "DecisionTree(entropy)  Accuracy = 0.9653\n",
            "ANN(MLP)               Accuracy = 0.9769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Generate synthetic Customer Segmentation dataset\n",
        "# --------------------------------------------------\n",
        "# Features: Age, Annual Income, Spending Score\n",
        "X, _ = make_blobs(\n",
        "    n_samples=300,\n",
        "    centers=5,\n",
        "    n_features=3,\n",
        "    cluster_std=2.5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "df = pd.DataFrame(X, columns=[\"Age\", \"Annual_Income\", \"Spending_Score\"])\n",
        "\n",
        "print(\"Customer dataset shape:\", df.shape)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Prepare data\n",
        "# --------------------------------------------------\n",
        "X = df.copy()\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "X = imputer.fit_transform(X)\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Define scalers\n",
        "# --------------------------------------------------\n",
        "SCALERS = {\n",
        "    \"StandardScaler\": StandardScaler(),\n",
        "    \"MinMaxScaler\": MinMaxScaler(),\n",
        "    \"RobustScaler\": RobustScaler(),\n",
        "    \"MaxAbsScaler\": MaxAbsScaler()\n",
        "}\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Apply KMeans with each scaler\n",
        "# --------------------------------------------------\n",
        "print(\"\\nScaler Comparison (KMeans, k=5):\\n\")\n",
        "\n",
        "for scaler_name, scaler in SCALERS.items():\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "    sil = silhouette_score(X_scaled, labels)\n",
        "    inertia = kmeans.inertia_\n",
        "\n",
        "    print(f\"{scaler_name:15s}  Silhouette = {sil:.4f}   Inertia = {inertia:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmklaWrv3dGa",
        "outputId": "66aa06a7-9582-442e-8cac-644610136596"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer dataset shape: (300, 3)\n",
            "\n",
            "Scaler Comparison (KMeans, k=5):\n",
            "\n",
            "StandardScaler   Silhouette = 0.4095   Inertia = 112.31\n",
            "MinMaxScaler     Silhouette = 0.3998   Inertia = 5.96\n",
            "RobustScaler     Silhouette = 0.4277   Inertia = 40.06\n",
            "MaxAbsScaler     Silhouette = 0.3936   Inertia = 21.47\n"
          ]
        }
      ]
    }
  ]
}